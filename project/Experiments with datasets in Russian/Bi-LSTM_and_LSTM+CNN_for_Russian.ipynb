{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bidirectional_lstm_imdb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHOci5yU4k54"
      },
      "source": [
        "#RuSentiment Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8P5Uo3colSi",
        "outputId": "c60cb812-87fc-4d01-e3b3-6e34f7612aec"
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkTdTgSQMnge"
      },
      "source": [
        "max_features = 20000  # Only consider the top 20k words\n",
        "maxlen = 20  # Only consider the first 200 words of text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_U7NNOZon9i"
      },
      "source": [
        "df_preselected = pd.read_csv('rusentiment_preselected_posts.csv')\n",
        "df_random = pd.read_csv('rusentiment_random_posts.csv')\n",
        "\n",
        "df_train = pd.concat([df_preselected, df_random]).reset_index()\n",
        "df_test = pd.read_csv('rusentiment_test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdEw037FB8pb"
      },
      "source": [
        "def data_preprocessing(df, fq = {}):\n",
        "  df_np = df[(df['label']=='positive')|(df['label']=='negative')]\n",
        "  text_tokenized = []\n",
        "  all_wf = []\n",
        "  for text in df_np['text']:\n",
        "    text_tokens = word_tokenize(text)\n",
        "    text_tokenized.append(text_tokens)\n",
        "    all_wf.extend(text_tokens)\n",
        "  df_np['text_tokenized'] = text_tokenized\n",
        "  fq_dict = defaultdict(int)\n",
        "  for wf in all_wf:\n",
        "    fq_dict[wf] += 1\n",
        "  if len(fq) > 0:\n",
        "    fq_dict = fq\n",
        "  x = []\n",
        "  for text_tokens in df_np['text_tokenized']:\n",
        "    emb = []\n",
        "    for w in text_tokens:\n",
        "      emb.append(fq_dict[w])\n",
        "    x.append(emb)\n",
        "  y = []\n",
        "  for label in df_np['label']:\n",
        "    if label=='negative':\n",
        "      y.append(0)\n",
        "    elif label=='positive':\n",
        "      y.append(1)\n",
        "  return x, y, fq_dict"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hJtmjstCHF5",
        "outputId": "cc5f92e8-ec13-43f3-dead-720974bc0ad3"
      },
      "source": [
        "x_train_ru, y_train_ru, fq = data_preprocessing(df_train)\n",
        "x_test_ru, y_test_ru, fq = data_preprocessing(df_test, fq)\n",
        "\n",
        "print(len(x_train_ru), \"Training sequences\")\n",
        "print(len(x_test_ru), \"Validation sequences\")\n",
        "x_train_ru = np.array(keras.preprocessing.sequence.pad_sequences(x_train_ru, maxlen=maxlen))\n",
        "x_test_ru = np.array(keras.preprocessing.sequence.pad_sequences(x_test_ru, maxlen=maxlen))\n",
        "y_train_ru = np.array(y_train_ru)\n",
        "y_test_ru = np.array(y_test_ru)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9764 Training sequences\n",
            "794 Validation sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmwt5yaZoi-t"
      },
      "source": [
        "## BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qGcTSEZoi-t"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "max_features = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RsBn8twoi-v"
      },
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "#model.summary()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu1WFdZpD_17"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYkF1Z-hDC87",
        "outputId": "9bdbebe4-da8b-4cfa-e4f7-e0118b37db53"
      },
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", f1_m, precision_m, recall_m])\n",
        "model.fit(x_train_ru, y_train_ru, batch_size=32, epochs=2, validation_data=(x_test_ru, y_test_ru))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "306/306 [==============================] - 31s 82ms/step - loss: 0.5637 - accuracy: 0.7053 - f1_m: 0.7828 - precision_m: 0.7231 - recall_m: 0.8811 - val_loss: 0.4339 - val_accuracy: 0.7872 - val_f1_m: 0.8537 - val_precision_m: 0.7995 - val_recall_m: 0.9201\n",
            "Epoch 2/2\n",
            "306/306 [==============================] - 23s 76ms/step - loss: 0.4880 - accuracy: 0.7598 - f1_m: 0.8108 - precision_m: 0.7964 - recall_m: 0.8357 - val_loss: 0.4387 - val_accuracy: 0.7922 - val_f1_m: 0.8578 - val_precision_m: 0.7999 - val_recall_m: 0.9295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66bdd83550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqbmv_Fmoi-y"
      },
      "source": [
        "##CNN+LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM_emb2jtoak"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# Embedding\n",
        "max_features = 20000\n",
        "maxlen = 100\n",
        "embedding_size = 128\n",
        "\n",
        "# Convolution\n",
        "kernel_size = 5\n",
        "filters = 64\n",
        "pool_size = 4\n",
        "\n",
        "# LSTM\n",
        "lstm_output_size = 70\n",
        "\n",
        "# Training\n",
        "batch_size = 30\n",
        "epochs = 2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(MaxPooling1D(pool_size=pool_size))\n",
        "model.add(LSTM(lstm_output_size))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKwgE59vM8os",
        "outputId": "7ff3fb6a-5450-4ec9-9abd-10ab29a65440"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=[\"accuracy\", f1_m, precision_m, recall_m])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(np.array(x_train_ru), np.array(y_train_ru),\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(np.array(x_test_ru), np.array(y_test_ru)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train...\n",
            "Epoch 1/2\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 20).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 20).\n",
            "325/326 [============================>.] - ETA: 0s - loss: 0.5733 - accuracy: 0.6909 - f1_m: 0.7814 - precision_m: 0.7097 - recall_m: 0.8946WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 20).\n",
            "326/326 [==============================] - 14s 37ms/step - loss: 0.5730 - accuracy: 0.6911 - f1_m: 0.7815 - precision_m: 0.7100 - recall_m: 0.8944 - val_loss: 0.4315 - val_accuracy: 0.8035 - val_f1_m: 0.8566 - val_precision_m: 0.8452 - val_recall_m: 0.8763\n",
            "Epoch 2/2\n",
            "326/326 [==============================] - 12s 36ms/step - loss: 0.4853 - accuracy: 0.7604 - f1_m: 0.8075 - precision_m: 0.7837 - recall_m: 0.8421 - val_loss: 0.4224 - val_accuracy: 0.8048 - val_f1_m: 0.8619 - val_precision_m: 0.8248 - val_recall_m: 0.9085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66b976c630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}