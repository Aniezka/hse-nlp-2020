{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1051)>\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import simplejson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import RAKE\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "from summa import keywords\n",
    "from gensim.summarization import keywords as kw\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "rake = RAKE.Rake(stop)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = MorphAnalyzer()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace(\"\\n\", \" \").replace('/', ' ')\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    lemmas = [lemmatizer.parse(t)[0].normal_form for t in text.split()]\n",
    "    lemmas = [i for i in lemmas if not i.isdigit()]\n",
    "    return ' '.join(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = []\n",
    "\n",
    "with open(\"Office_Products.txt\", \"r\") as f:\n",
    "    entry = {}\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        colonPos = line.find(':')\n",
    "        if colonPos == -1:\n",
    "            buf.append(entry)\n",
    "            entry = {}\n",
    "            continue\n",
    "        eName = line[:colonPos]\n",
    "        rest = line[colonPos+2:]\n",
    "        entry[eName] = rest\n",
    "    buf.append(entry)\n",
    "    \n",
    "data = pd.DataFrame.from_dict(buf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product/price</th>\n",
       "      <th>product/productId</th>\n",
       "      <th>product/title</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/profileName</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "      <th>review/time</th>\n",
       "      <th>review/userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unknown</td>\n",
       "      <td>B000E7F8LA</td>\n",
       "      <td>Low Odor Dry Erase Markers, Vibrant DryGuard I...</td>\n",
       "      <td>0/0</td>\n",
       "      <td>S. Young</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Boone Dry Erase Markers</td>\n",
       "      <td>I was really happy to find out that I was stil...</td>\n",
       "      <td>1214784000</td>\n",
       "      <td>A266N1TVOHUG8V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.94</td>\n",
       "      <td>B000CD483K</td>\n",
       "      <td>C-Line Clear 62033 Heavyweight Antimicrobial P...</td>\n",
       "      <td>0/0</td>\n",
       "      <td>Thomas Perrin \"Perrin &amp; Treggett\"</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Superior product for archival storage</td>\n",
       "      <td>Ever since some of my important family documen...</td>\n",
       "      <td>1353628800</td>\n",
       "      <td>A1186EZQ23CU4X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>443.04</td>\n",
       "      <td>B0006Q9950</td>\n",
       "      <td>Wasp Barcode Technologies 633808920128 Cordles...</td>\n",
       "      <td>14/14</td>\n",
       "      <td>Handyman</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good product, bad instructions</td>\n",
       "      <td>My boss had us using cheap USB barcode scanner...</td>\n",
       "      <td>1320364800</td>\n",
       "      <td>A2CW9GKMNFAU6R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.99</td>\n",
       "      <td>B0001YXWV4</td>\n",
       "      <td>Panasonic MARKER ERASER KIT 1 EA-BLK RED BLU E...</td>\n",
       "      <td>0/0</td>\n",
       "      <td>C L Huddleston</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best markers made</td>\n",
       "      <td>We use our white boards every day. Tired of ma...</td>\n",
       "      <td>1359676800</td>\n",
       "      <td>A14XEQHPPULFDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.99</td>\n",
       "      <td>B0001YXWV4</td>\n",
       "      <td>Panasonic MARKER ERASER KIT 1 EA-BLK RED BLU E...</td>\n",
       "      <td>0/0</td>\n",
       "      <td>Eiji Nakamura</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Good item</td>\n",
       "      <td>Fast shipment and fast response.The item is go...</td>\n",
       "      <td>1358294400</td>\n",
       "      <td>A7YN96KKCI8GO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product/price product/productId  \\\n",
       "0       unknown        B000E7F8LA   \n",
       "1         17.94        B000CD483K   \n",
       "2        443.04        B0006Q9950   \n",
       "3         13.99        B0001YXWV4   \n",
       "4         13.99        B0001YXWV4   \n",
       "\n",
       "                                       product/title review/helpfulness  \\\n",
       "0  Low Odor Dry Erase Markers, Vibrant DryGuard I...                0/0   \n",
       "1  C-Line Clear 62033 Heavyweight Antimicrobial P...                0/0   \n",
       "2  Wasp Barcode Technologies 633808920128 Cordles...              14/14   \n",
       "3  Panasonic MARKER ERASER KIT 1 EA-BLK RED BLU E...                0/0   \n",
       "4  Panasonic MARKER ERASER KIT 1 EA-BLK RED BLU E...                0/0   \n",
       "\n",
       "                  review/profileName review/score  \\\n",
       "0                           S. Young          4.0   \n",
       "1  Thomas Perrin \"Perrin & Treggett\"          5.0   \n",
       "2                           Handyman          4.0   \n",
       "3                     C L Huddleston          5.0   \n",
       "4                      Eiji Nakamura          5.0   \n",
       "\n",
       "                          review/summary  \\\n",
       "0                Boone Dry Erase Markers   \n",
       "1  Superior product for archival storage   \n",
       "2         Good product, bad instructions   \n",
       "3                      Best markers made   \n",
       "4                              Good item   \n",
       "\n",
       "                                         review/text review/time  \\\n",
       "0  I was really happy to find out that I was stil...  1214784000   \n",
       "1  Ever since some of my important family documen...  1353628800   \n",
       "2  My boss had us using cheap USB barcode scanner...  1320364800   \n",
       "3  We use our white boards every day. Tired of ma...  1359676800   \n",
       "4  Fast shipment and fast response.The item is go...  1358294400   \n",
       "\n",
       "    review/userId  \n",
       "0  A266N1TVOHUG8V  \n",
       "1  A1186EZQ23CU4X  \n",
       "2  A2CW9GKMNFAU6R  \n",
       "3  A14XEQHPPULFDA  \n",
       "4   A7YN96KKCI8GO  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['normal_title'] = data['product/title'].apply(normalize_text)\n",
    "data['normal_text'] = data['review/text'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3 балла) Предложите 3 способа найти упоминания товаров в отзывах. Например, использовать bootstrapping: составить шаблоны вида \"холодильник XXX\", найти все соответствующие n-граммы и выделить из них называние товара. Могут помочь заголовки и дополнительные данные с Amazon (Metadata здесь) Какие данные необходимы для каждого из способов? Какие есть достоинства/недостатки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) (будет реализован) Взять слова из title каждого товара и найти для них все пары употребления (ообычно в заглавии содержатся основные слова: название модели, фирма, общее назначение, категория и т. п.) и собрать употребляющиеся с ним рядом слова из отзыва (перед этим нормализовав все тексты). Достоинствами метода является хорошая интерпретируемость и регулирование выделяемых n-грамм. В качестве минусов можно выделить отсутствие регулируемых параметров (но можно использовать параллельно с другими способами, которые даже в совокупности можно легко реализовать).\n",
    "\n",
    "2) Подход, основанных на правилах, в котором необходимо построить правила и с помощью парсера (который выделял бы нужные элементы из текста) извлечь соответствующие элементы. Могло бы помочь описание/свойства устройства. Достоинствами метода являются регулируемые правила, которые позволяют доставать конкретные параметры устройства и лёгкая формализация. Минусами данного подхода являются отсутствие знаний о метаданных и структурах отзывов, что повлечёт за собой изучение исходных примеров для выявления работающих правил.\n",
    "\n",
    "3) Использование готовых решений из библиотек, основанных на нейросетях. Достоинствами такого подхода являются нахождение сложных зависимостей и большое количество регулируемых параметров (например, для каждой конкретной категории). Минусами являются скорость работы и невозможность интерпретировать подход к нахождению сущностей, также зачастую будут выделятся только named entity, а общие слова (принтер, сканер и т п) будут пропускаться.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 балла) Реализуйте один из предложенных вами способов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 балл) Соберите n-граммы с полученными сущностями (NE + левый сосед / NE + правый сосед)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocations = []\n",
    "id_keywords = {}\n",
    "\n",
    "for name in data['product/productId'].unique():\n",
    "    keywords = data['normal_title'][data['product/productId'] == name].values[0].split()\n",
    "    review_words = data['normal_text'][data['product/productId'] == name].values[0].split()\n",
    "    \n",
    "#     print(review_words)\n",
    "    for ind, word in enumerate(review_words):\n",
    "        if word in keywords:\n",
    "            if 1 < ind:\n",
    "                collocations.append(review_words[ind - 1] + \" \" + word)\n",
    "            if ind < len(review_words) - 2:\n",
    "                collocations.append(word + \" \" + review_words[ind + 1])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим все уникальные коллокации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9103"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_collocations = set(collocations)\n",
    "len(unique_collocations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3 балла) Ранжируйте n-граммы с помощью 3 коллокационных метрик (t-score, PMI и т.д.). Не забудьте про частотный фильтр / сглаживание. Выберите лучший результат (какая метрика ранжирует выше коллокации, подходящие для отчёта)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем необходимые классы и функцию, которая возвращает только те биграммы, которые мы выделили из отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = BigramAssocMeasures()\n",
    "bigram_finder = BigramCollocationFinder.from_documents([i.split() for i in data['normal_text'].values])\n",
    "bigram_finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_our_bigram(my_set, finder):\n",
    "    res = []\n",
    "    for value in finder:\n",
    "        if value[0][0] + ' ' + value[0][1] in my_set:\n",
    "            res.append(value)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показать view_top_n экземпляров с наивысшим скоором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_top_n = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### likelihood ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('if', 'you'), 20165.278169880785),\n",
       " (('i', 'have'), 18010.605080651658),\n",
       " (('this', 'phone'), 14421.019309193884),\n",
       " (('caller', 'id'), 14246.752822223207),\n",
       " (('easy', 'to'), 13129.146250369971),\n",
       " (('on', 'the'), 12016.587579175814),\n",
       " (('of', 'the'), 11534.601231167548),\n",
       " (('it', 'is'), 10788.412438246625),\n",
       " (('i', 'bought'), 10713.261325363601),\n",
       " (('a', 'lot'), 9871.753396066808),\n",
       " (('answering', 'machine'), 9678.644699830651),\n",
       " (('to', 'use'), 9548.964043632008),\n",
       " (('to', 'be'), 8380.503721501605),\n",
       " (('a', 'few'), 7670.070973270589),\n",
       " (('in', 'the'), 7581.8427298064125)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_finder_like = bigram_finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "get_our_bigram(unique_collocations, bigram_finder_like)[:view_top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('van', 'gogh'), 15.96602304429172),\n",
       " (('vision', 'elite'), 15.398982451567827),\n",
       " (('hello', 'kitty'), 14.75852483825497),\n",
       " (('obus', 'forme'), 14.257626602322286),\n",
       " (('stainless', 'steel'), 13.907841973203196),\n",
       " (('movie', 'writer'), 13.32150369889011),\n",
       " (('dr', 'grip'), 12.805031167619418),\n",
       " (('fellowes', 'powershred'), 12.695375454348055),\n",
       " (('bubble', 'wrap'), 12.65519265828769),\n",
       " (('double', 'sided'), 12.6000247036674),\n",
       " (('step', 'stool'), 12.591627529510221),\n",
       " (('hearing', 'aid'), 12.166321694777553),\n",
       " (('poly', 'mailers'), 12.10620070233998),\n",
       " (('coin', 'sorter'), 12.099737793771757),\n",
       " (('filing', 'cabinet'), 12.044746035977905)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_finder_pmi = bigram_finder.score_ngrams(bigram_measures.pmi)\n",
    "get_our_bigram(unique_collocations, bigram_finder_pmi)[:view_top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('van', 'gogh'), 0.851063829787234),\n",
       " (('caller', 'id'), 0.70893760539629),\n",
       " (('obus', 'forme'), 0.6181818181818182),\n",
       " (('stainless', 'steel'), 0.6122448979591837),\n",
       " (('answering', 'machine'), 0.5532533624136677),\n",
       " (('vision', 'elite'), 0.4878048780487805),\n",
       " (('laser', 'pointer'), 0.47173489278752434),\n",
       " (('hello', 'kitty'), 0.4657534246575342),\n",
       " (('customer', 'service'), 0.45029624753127057),\n",
       " (('movie', 'writer'), 0.4142857142857143),\n",
       " (('heavy', 'duty'), 0.40877598152424943),\n",
       " (('dry', 'erase'), 0.3744493392070485),\n",
       " (('mouse', 'pad'), 0.3397152675503191),\n",
       " (('bubble', 'wrap'), 0.32044198895027626),\n",
       " (('step', 'stool'), 0.32038834951456313)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_finder_dice = bigram_finder.score_ngrams(bigram_measures.dice)\n",
    "get_our_bigram(unique_collocations, bigram_finder_dice)[:view_top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Хи-квадрат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('van', 'gogh'), 0.851063829787234),\n",
       " (('caller', 'id'), 0.70893760539629),\n",
       " (('obus', 'forme'), 0.6181818181818182),\n",
       " (('stainless', 'steel'), 0.6122448979591837),\n",
       " (('answering', 'machine'), 0.5532533624136677),\n",
       " (('vision', 'elite'), 0.4878048780487805),\n",
       " (('laser', 'pointer'), 0.47173489278752434),\n",
       " (('hello', 'kitty'), 0.4657534246575342),\n",
       " (('customer', 'service'), 0.45029624753127057),\n",
       " (('movie', 'writer'), 0.4142857142857143),\n",
       " (('heavy', 'duty'), 0.40877598152424943),\n",
       " (('dry', 'erase'), 0.3744493392070485),\n",
       " (('mouse', 'pad'), 0.3397152675503191),\n",
       " (('bubble', 'wrap'), 0.32044198895027626),\n",
       " (('step', 'stool'), 0.32038834951456313)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_finder_chi_sq = bigram_finder.score_ngrams(bigram_measures.chi_sq)\n",
    "get_our_bigram(unique_collocations, bigram_finder_dice)[:view_top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиблее качественными выглядят результаты работы Dice, выделяется больше качественных сочетаний и показывается информация о конкретных моделях и фирмах, likelihood выделяет по частотности слов и, как следствие, биграммы со стоп-словами попадают в топ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 балл) Сгруппируйте полученные коллокации по NE, выведите примеры для 5 товаров. Должны получиться примерно такие группы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_by_word(my_set, finder, word):\n",
    "    res = []\n",
    "    for value in finder:\n",
    "        if value[0][0] == word or value[0][1] == word:\n",
    "            res.append(value[0][0] + \" \" + value[0][1])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word:  sharpener\n",
      "pencil sharpener\n",
      "electric sharpener\n",
      "xacto sharpener\n",
      "this sharpener\n",
      "sharpener has\n",
      "\n",
      "\n",
      "Word:  card\n",
      "credit card\n",
      "card stock\n",
      "business card\n",
      "bookman card\n",
      "card holder\n",
      "\n",
      "\n",
      "Word:  printer\n",
      "laser printer\n",
      "inkjet printer\n",
      "hp printer\n",
      "canon printer\n",
      "photo printer\n",
      "\n",
      "\n",
      "Word:  phone\n",
      "this phone\n",
      "cordless phone\n",
      "the phone\n",
      "speaker phone\n",
      "phone system\n",
      "\n",
      "\n",
      "Word:  pencil\n",
      "pencil sharpener\n",
      "electric pencil\n",
      "pencil sharpeners\n",
      "mechanical pencil\n",
      "pencil case\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['sharpener', 'card', 'printer', 'phone', 'pencil']\n",
    "get_top_n = 5\n",
    "\n",
    "for word in words:\n",
    "    print(\"\\nWord: \", word)\n",
    "    print(*get_top_by_word(unique_collocations, bigram_finder_dice, word)[:get_top_n], sep='\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус (2 балла): если придумаете способ объединить синонимичные упоминания (например, \"Samsung Galaxy Watch\", \"watch\", \"smartwatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синонимичные упоминания можно детектировать с помощью расстояния между векторами, если получить эмбеддинги соответствующих слов (word2vec, fasttext и т п), но проблема в редких употреблениях некоторых специфичных слов в предобученных моделях (smartwatch и watch) - но это проблему можно решить дообучением на интересующих текстах. Также употребление прилагательных, глаголов можно определить с помощью методов выделения частей речи, что позволит фильтровать такие сочетания (NOUN + VERB/ADJ) и брать из них только существительное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
